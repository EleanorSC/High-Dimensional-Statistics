---
title: "Multidimensional Data"
subtitle: "DTP Intermediate Statistics and Data Management 2024"
author: "Dr Eleanor Conole"
date: "28th November 2024"
output:
  html_document:
    theme: flatly
    highlight: rstudio
    toc: true
    toc_float: true
---

## Setup

Ensure you have completed any necessary setup instructions before proceeding.

```{r,warning=FALSE, message=FALSE, echo = FALSE}
# Set a CRAN mirror (important for downloading packages)
options(repos = c(CRAN = "https://cran.rstudio.com/"))
```

```{r,warning=FALSE, message=FALSE}
# Install BiocManager
install.packages("BiocManager", quiet = TRUE)

# Download and read dependencies file
download.file(
  "https://raw.githubusercontent.com/EleanorSC/High-Dimensional-Statistics/main/dependencies.csv",
  destfile = 'dependencies.csv'
)
table <- read.table('dependencies.csv')

# Install dependencies using BiocManager
BiocManager::install(table[[1]])

# Create a directory for data files
dir.create("data", showWarnings = FALSE)

# List of data files to download
data_files <- c(
  "coefHorvath.rds",
  "methylation.rds",
  "prostate.rds"
)

# Download data files into the "data" directory
for (file in data_files) {
  download.file(
    url = file.path(
      "https://raw.githubusercontent.com/EleanorSC/High-Dimensional-Statistics/main/Data",
      file
    ),
    destfile = file.path("data", file)
  )
}
```

## Part 1: Intro to High Dimensional Data


### Challenge 1

Descriptions of four research questions and their datasets are given below. Which of these scenarios use high-dimensional data?

1. Predicting patient blood pressure using: cholesterol level in blood, age, and BMI measurements, collected from 100 patients.
2. Predicting patient blood pressure using: cholesterol level in blood, age, and BMI, as well as information on 200,000 single nucleotide polymorphisms from 100 patients.
3. Predicting the length of time patients spend in hospital with pneumonia infection using: measurements on age, BMI, length of time with symptoms, number of symptoms, and percentage of neutrophils in blood, using data from 200 patients.
4. Predicting probability of a patient’s cancer progressing using gene expression data from 20,000 genes, as well as data associated with general patient health (age, weight, BMI, blood pressure) and cancer growth (tumour size, localised spread, blood test results).

### Challenge 2

 Load the dataset using the `here` package. 
 Ensure the `prostate.rds` file is located in the `data` folder of your project directory.
 
```{r,warning=FALSE, message=FALSE, echo = FALSE}
library(here)
```
 
```{r,warning=FALSE, message=FALSE, echo = TRUE}
prostate <- readRDS(here("data/prostate.rds"))
```

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------
# Examine the Dataset
# ----------------------------------------------------------------------

# The dataset represents clinical data where each row corresponds to a single patient.
# Let's first examine the dimensions of the dataset to understand its structure.

# Determine the number of observations (n) and features (p)
dimensions <- dim(prostate)  # Returns a vector: [number of rows, number of columns]
print(dimensions)  # Print the dimensions

# Examine the variables measured in the dataset
variable_names <- names(prostate)  # Returns the column names (variables)
print(variable_names)

# View the first few rows of the dataset to get a sense of its contents
head(prostate)

# ----------------------------------------------------------------------
# Visualize Relationships Between Variables
# ----------------------------------------------------------------------

# Plot pairwise relationships between variables using the `pairs()` function.
# This function creates a scatterplot matrix to visualize the relationships
# between all pairs of numeric variables.

pairs(prostate)
```

### Challenge 3

1). Use the `cor()` function to examine correlations between all variables in the `prostate` dataset. Are some pairs of variables highly correlated using a threshold of 0.75 for the correlation coefficients?

2). Use the `lm()` function to fit univariate regression models to predict patient age using two variables that are highly correlated as predictors. Which of these variables are statistically significant predictors of age? Hint: the `summary()` function can help here.

3). Fit a multiple linear regression model predicting patient age using both variables. What happened?

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------
# 1. Examine Correlations Between Variables
# ----------------------------------------------------------------------
# Use the cor() function to compute pairwise correlations between all variables 
# in the prostate dataset. Identify pairs of variables with a correlation
# coefficient greater than 0.75 (absolute value).

# Compute the correlation matrix for the dataset
cor(prostate)

# rounding helps to visualise the correlations
round(cor(prostate), 2) 
```


```{r,warning=FALSE, message=FALSE, echo = TRUE}
# As seen above, some variables are highly correlated.
# In particular, the correlation between gleason and pgg45 is equal to 0.75

cor_matrix <- cor(prostate, use = "complete.obs")
high_correlations <- which(abs(cor_matrix) > 0.75 & abs(cor_matrix) < 1, arr.ind = TRUE)

# Extract and display the highly correlated variable pairs
high_cor_pairs <- tibble::tibble(
  Var1 = rownames(cor_matrix)[high_correlations[, 1]],
  Var2 = colnames(cor_matrix)[high_correlations[, 2]],
  Correlation = cor_matrix[high_correlations]
)
print(high_cor_pairs)
```

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------
# 2. Fit Univariate Regression Models
# ----------------------------------------------------------------------
# Fitting univariate regression models to predict age using `gleason` and `pgg45` as predictors.
# Use lm() to fit univariate regression models predicting patient age 
# using two highly correlated variables. Evaluate statistical significance
# of each predictor using summary().

model_gleason <- lm(age ~ gleason, data = prostate)
model_pgg45 <- lm(age ~ pgg45, data = prostate)

# Summarize results for each model
summary(model_gleason)  # Check significance of gleason
summary(model_pgg45)  # Check significance of pgg45

# ----------------------------------------------------------------------
# 3. Fit a Multiple Regression Model
# ----------------------------------------------------------------------
# Fit a multiple regression model predicting patient age using both variables.
# Examine what happens when both correlated variables are included.

# Fit a multiple regression model
model_multivar <- lm(age ~ gleason + pgg45, data = prostate)

# Summarize the multiple regression model
summary(model_multivar)

```
### Using Bioconductor

This lecture focuses on statistical methods for visualising and analysing high-dimensional biological data using Bioconductor packages.

Bioconductor is an open-source platform designed for analysing high-throughput genomic data. It provides a variety of useful packages and example datasets.More details and resources are available at: https://www.bioconductor.org/

Bioconductor packages can be installed and managed using the `BiocManager` package.

```{r,warning=FALSE, message=FALSE, echo = TRUE, results="hide"}

# Let's load the "minfi" package, a Bioconductor package specifically designed
# for analyzing Illumina Infinium DNA methylation arrays.
# BiocManager::install("minfi")

library("minfi")
browseVignettes("minfi")

#methylation <- readRDS("methylation.rds")
methylation <- readRDS(here("data/methylation.rds"))
head(colData(methylation))
```

```{r,warning=FALSE, message=FALSE, echo = TRUE}

methyl_mat <- t(assay(methylation))
## calculate correlations between cells in matrix
cor_mat <- cor(methyl_mat)

cor_mat[1:10, 1:10] # print the top-left corner of the correlation matrix
```

## Part 2: Regression with many outcomes using DNAm data

```{r,warning=FALSE, message=FALSE, echo = TRUE}
library("here")
library("minfi")
methylation <- readRDS(here("data/methylation.rds"))

# Check the dimensions of the dataset using dim().
# Note: In computational biology data structures in R, observations are stored as columns 
# and features (e.g., genomic sites) are stored as rows.
# This contrasts with typical tabular data where features are columns and observations are rows.

# Access assay data (e.g., normalised methylation levels) using assay().

# Retrieve sample-level information using colData().
dim(methylation)

# The output shows that the object has dimensions of 5000 × 37,
# indicating 5000 features and 37 observations.
# To extract the matrix of methylation M-values, use the assay() function.

methyl_mat <- assay(methylation)

hist(methyl_mat, xlab = "M-value")

head(colData(methylation))

# Association between age and DNA methylation:
# The following heatmap summarises age and methylation levels available in the methylation dataset:

library("ComplexHeatmap")

age <- methylation$Age

# sort methylation values by age 
order <- order(age)
age_ord <- age[order]
methyl_mat_ord <- methyl_mat[, order]

# plot heatmap
Heatmap(methyl_mat_ord,
        cluster_columns = FALSE,
        show_row_names = FALSE,
        show_column_names = FALSE,
        name = "M-value",
        row_title = "Feature", 
        column_title =  "Sample", 
        top_annotation = columnAnnotation(age = age_ord))
```
        
### Challenge 1

 Why can't we simply fit many linear regression models for every combination of features (colData and assays)
 and draw conclusions based on significant p-values?

 There are several problems with this approach:

 1. Multiple Testing Problem:
    - If we perform 5000 tests for each of 14 variables, even with no true associations,
      random noise would produce some significant (spurious) results.

 2. Small Sample Size:
    - Some covariates may have very small sample sizes (e.g., certain ethnicities),
      leading to unreliable or spurious findings.

 3. Lack of Research Focus:
    - Without a clear research question, interpreting each model becomes ambiguous.
    - Rationalising findings after the fact can lead to creating unsupported "stories"
      based solely on significant results.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------
# CHALLENGE 1
# ----------------------------------------------------------------------
# Arbitrarily select the first CpG in the methyl_mat matrix (the one on its first row):

age <- methylation$Age

# methyl_mat[1, ] indicates that the 1st CpG will be used as outcome variable
lm_age_methyl1 <- lm(methyl_mat[1, ] ~ age)
lm_age_methyl1

# We now have estimates for the expected methylation level when age equals 0 (the intercept) 
# and the change in methylation level for a unit change in age (the slope).
# We could plot this linear model:

plot(age, methyl_mat[1, ], xlab = "Age", ylab = "Methylation level", pch = 16)
abline(lm_age_methyl1)

# For this linear model, we can use tidy() from the broom package to extract detailed
# information about the coefficients and the associated hypothesis tests in this model:

library("broom")
tidy(lm_age_methyl1)
```

### Challenge 2

1. In the model we fitted, the estimate for the intercept is 0.902 and its associated p-value is 0.0129. What does this mean?

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------
# CHALLENGE 2
# ----------------------------------------------------------------------

# SOLUTION:
# The first coefficient, the intercept, represents the mean methylation value 
# for the first CpG when age is zero (estimated as 0.902).
# However, this is not meaningful since there are no observations with age zero 
# or below 20 in the dataset.
# The p-value tests whether the intercept (β₀) is zero, 
# but this is not relevant since methylation levels at age zero are not of interest.
# The key focus is on the regression coefficient for age, 
# as it shows whether there is a linear relationship between age and methylation.
```

### Using limma
```{r,warning=FALSE, message=FALSE, echo = TRUE}
# What is a model matrix?

design_age <- model.matrix(lm_age_methyl1) # model matrix
head(design_age)
dim(design_age)

# The model matrix has the same number of rows as the methylation data has samples.
# It has two columns: one for the intercept (like in the linear model we fit earlier)
# and one for age.

# When using lm(), this step happens automatically, but here we specify the model matrix directly.
# For more complex experimental designs, the limma user manual provides guidance on creating model matrices.
# In this case, we use a simple two-variable model.

# We use the lmFit() function to fit the model, passing the methylation data and model matrix.
# Internally, lmFit() efficiently runs lm() for each row of the data.

# The eBayes() function, applied to the lmFit() output, performs pooled standard error estimation,
# resulting in moderated t-statistics and p-values.

library("limma")
design_age <- model.matrix(lm_age_methyl1) # model matrix
fit_age <- lmFit(methyl_mat, design = design_age)
fit_age <- eBayes(fit_age)

# Use the topTable() function to obtain the results of the linear models.
# By default, topTable() returns results for the first coefficient in the model.
# The first coefficient corresponds to the intercept term, which is not of interest here.
# Specify coef = 2 to focus on the second coefficient (age in this case).

# By default, topTable() returns only the top 10 results.
# To see all results, set number = nrow(fit_age), ensuring a row for every input row.

toptab_age <- topTable(fit_age, coef = 2, number = nrow(fit_age))
head(toptab_age)

# The output of topTable includes several columns:
# - logFC: The coefficient estimate (log fold change), representing effect size.
# - aveExpr: The average expression level.
# - t: The t-statistic for the coefficient.
# - P.Value: The p-value for the test.
# - adj.P.Val: The adjusted p-value (we'll discuss adjusted p-values shortly).
# - B: The log-odds that a feature is significantly different (a transformation of the p-value, not covered here).
# The term logFC is used for historical reasons from microarray experiments.

# These results provide effect sizes and p-values for the association between methylation levels
# at each locus and age across the 37 samples.

# To visualise effect sizes (coefficients) and statistical significance (p-values),
# we can plot effect sizes against p-values for all linear models.
# Such plots are called "volcano plots" because their shape resembles an eruption.

plot(toptab_age$logFC, -log10(toptab_age$P.Value),
     xlab = "Effect size", ylab = bquote(-log[10](p-value)),
     pch = 19
)

# In this figure:
# - Each point represents a feature of interest.
# - The x-axis shows the effect size from a linear model.
# - The y-axis shows −log10(p-value), where higher values indicate stronger statistical evidence
#   of a non-zero effect size.

# - Positive effect sizes indicate increasing methylation with age.
# - Negative effect sizes indicate decreasing methylation with age.
# - Points higher on the y-axis represent features with results unlikely under the null hypothesis.

# The goal is to identify features with different methylation levels across age groups.
# Ideally, there would be a clear separation between “null” (no effect) and “non-null” (effect exists) features.
# However, in practice, we often see a continuum of effect sizes and p-values without clear separation.

# Statistical methods can provide insights from these continuous measures,
# but it is often useful to generate a list of features with confident non-zero effect sizes.
# This is challenging due to the large number of tests performed.
```

### Challenge 3

1. Try fitting a linear model using smoking status as a covariate instead of age,
and create a volcano plot. Note: Smoking status is stored as `methylation$smoker`.

2. In the lecture example, we saw that information sharing can result in larger p-values.
Why might this be preferable?

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------
# CHALLENGE 3
# ----------------------------------------------------------------------

# 1. 
design_smoke <- model.matrix(~methylation$smoker)
fit_smoke <- lmFit(methyl_mat, design = design_smoke)
fit_smoke <- eBayes(fit_smoke)
toptab_smoke <- topTable(fit_smoke, coef = 2, number = nrow(fit_smoke))
plot(toptab_smoke$logFC, -log10(toptab_smoke$P.Value),
     xlab = "Effect size", ylab = bquote(-log[10](p)),
     pch = 19
)

# 2. 
# Being more conservative when identifying features can help reduce false discoveries.
# It is also important to be cautious when rejecting the null hypothesis based on 
# small standard errors caused by abnormally low variability for certain features.

```

### The problem with multiple tests

With a large number of features, it is important to determine which features are "interesting" or "significant" for further study.

Using a standard significance threshold of 0.05 may lead to many false positives. A p-value threshold of 0.05 means there is a 1 in 20 chance of observing results as extreme or more extreme under the null hypothesis (no association between age and methylation).

If we perform many more than 20 tests, we are likely to observe significant p-values purely due to random chance, even when the null hypothesis is true.

To illustrate this, we can permute (scramble) the age values and rerun the test to see how random chance affects the results.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------

# Permute (scramble) the age values and rerun the test to see how random chance affects the results.

set.seed(123) 
age_perm <- age[sample(ncol(methyl_mat), ncol(methyl_mat))]
design_age_perm <- model.matrix(~age_perm)

fit_age_perm <- lmFit(methyl_mat, design = design_age_perm)
fit_age_perm <- eBayes(fit_age_perm)
toptab_age_perm <- topTable(fit_age_perm, coef = 2, number = nrow(fit_age_perm))

plot(toptab_age_perm$logFC, -log10(toptab_age_perm$P.Value),
     xlab = "Effect size", ylab = bquote(-log[10](p)),
     pch = 19
)
abline(h = -log10(0.05), lty = "dashed", col = "red")
```

A random sequence of ages was generated, so no true association between methylation levels and this random sequence is expected. Despite this, many features still show p-values below the traditional significance threshold of p = 0.05. In this example, 226 features are significant at p < 0.05, which demonstrates how using this threshold in a real experiment could falsely identify features as associated with age purely by chance.

When performing multiple tests, features are classified as either "significant" or "non-significant," but some classifications will inevitably be incorrect. Results fall into four categories:

- True Positive: Truly different and correctly labeled as different.
- False Negative: Truly different but incorrectly labeled as not different.
- False Positive (False Discovery): Not truly different but incorrectly labeled as different.
- True Negative: Not truly different and correctly labeled as not different.

At a significance level of 5%, 5% of results will be false positives (false discoveries) by chance, as p-values are uniformly distributed under the null hypothesis.

To control false discoveries, the Bonferroni correction can be applied:

Divide the significance threshold by the number of tests (n).
Alternatively, multiply the p-values by the number of tests. However, Bonferroni is very conservative, especially with a large number of features.

```{r,warning=FALSE, message=FALSE, echo = TRUE}

# BONFERRONI correction – which divides the significance level by 
# the number of tests performed (n);  Family-Wise Error Rate (fwer)

p_raw <- toptab_age$P.Value
p_fwer <- p.adjust(p_raw, method = "bonferroni")
plot(p_raw, p_fwer, pch = 16, log="xy")
abline(0:1, lty = "dashed")
abline(v = 0.05, lty = "dashed", col = "red")
abline(h = 0.05, lty = "dashed", col = "red")
```

### Challenge 4

1. At a significance level of 0.05, with 100 tests, what is the Bonferroni significance threshold?


```{r,warning=FALSE, message=FALSE, echo = TRUE}
# Threshold = 0.05 / 100  = 0.0005
```

2. In a gene expression experiment, after FDR correction with an adjusted p-value threshold of 0.05, 500 significant genes are observed. What proportion of these genes are truly different?

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------
# CHALLENGE 4
# ----------------------------------------------------------------------

# 2. We can’t say what proportion of these genes are truly different. However, if we repeated this
# experiment and statistical test over and over, on average 5% of the results from each run
# would be false discoveries.

# Try applying FDR correction to the p_raw vector.
# Hint: Use the p.adjust() function and check help("p.adjust") for details on the method.
# A = The following code runs FDR correction and compares it to non-corrected values and to Bonferroni:

p_fdr <- p.adjust(p_raw, method = "BH")
plot(p_raw, p_fdr, pch = 16, log="xy")
abline(0:1, lty = "dashed")
abline(v = 0.05, lty = "dashed", col = "red")
abline(h = 0.05, lty = "dashed", col = "red")

# plot of chunk plot-fdr-fwer
plot(p_fwer, p_fdr, pch = 16, log="xy")
abline(0:1, lty = "dashed")
abline(v = 0.05, lty = "dashed", col = "red")
abline(h = 0.05, lty = "dashed", col = "red")
```

## Part 3: Regularisation

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# ----------------------------------------------------------------------
# REGULARISATION
# ----------------------------------------------------------------------

library("here")
library("minfi")
methylation <- readRDS(here("data/methylation.rds"))

## here, we transpose the matrix to have features as rows and samples as columns
methyl_mat <- t(assay(methylation))
age <- methylation$Age
```

Then, we try to fit a model with outcome age and all 5,000 features in this dataset as predictors (average methylation levels, M-values, across different sites in the genome).

```{r,warning=FALSE, message=FALSE, results="hide"}
# by using methyl_mat in the formula below, R will run a multivariate regression
# model in which each of the columns in methyl_mat is used as a predictor. 
fit <- lm(age ~ methyl_mat)
```

```{r,warning=FALSE, message=FALSE, results="hide"}
summary(fit)
```

You can see that we’re able to get some effect size estimates, but they seem very high! The summary also says that we were unable to estimate effect sizes for 4,964 features because of “singularities”. We clarify what singularities are in the note below but this means that R couldn’t find a way to perform the calculations necessary to fit the model. Large effect sizes and singularities are common when naively fitting linear regression models with a large number of features (i.e., to high-dimensional data), often since the model cannot distinguish between the effects of many, correlated features or when we have more features than observations.

### Singularities

A singularity occurs when a matrix used in a linear model cannot be inverted, which is necessary for calculating the model's coefficients. This happens if:

There are more features (predictors) than observations.
The features are highly correlated.
In such cases, R cannot perform the required calculations and returns an error about singularities.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
xtx <- t(methyl_mat) %*% methyl_mat
det(xtx)
```

### Correlated features

In high-dimensional datasets, there are often multiple features that contain redundant information (correlated features). If we visualise the level of correlation between sites in the methylation dataset, we can see that many of the features represent the same information - there are many off-diagonal cells, which are deep red or blue. For example, the following heatmap visualises the correlations for the first 500 features in the methylation dataset (we selected 500 features only as it can be hard to visualise patterns when there are too many features!).

```{r,warning=FALSE, message=FALSE, echo = TRUE}
library("ComplexHeatmap")
small <- methyl_mat[, 1:500]
cor_mat <- cor(small)
Heatmap(cor_mat,
    column_title = "Feature-feature correlation in methylation data",
    name = "Pearson correlation",
    show_row_dend = FALSE, show_column_dend = FALSE,
    show_row_names = FALSE, show_column_names = FALSE
)
```

### Challenge 1
Consider or discuss in groups:
1. Why would we observe correlated features in high-dimensional biological data?
2. Why might correlated features be a problem when fitting linear models?
3. What issue might correlated features present when selecting features to include in a model one at a time?

1. Many of the features in biological data represent very similar information biologically. For example, sets of genes that form complexes are often expressed in very similar quantities. Similarly, methylation levels at nearby sites are often very highly correlated.

2. Correlated features can make inference unstable or even impossible mathematically.

3. When we are selecting features one at a time we want to pick the most predictive feature each time. When a lot of features are very similar but encode slightly different information, which of the correlated features we select to include can have a huge impact on the later stages of model selection!

### Model selection using training and test sets

Sets of models are often compared using statistics such as adjusted R^2, AIC or BIC. These show us how well the model is learning the data used in fitting that same model 1. However, these statistics do not really tell us how well the model will generalise to new data. This is an important thing to consider – if our model doesn’t generalise to new data, then there’s a chance that it’s just picking up on a technical or batch effect in our data, or simply some noise that happens to fit the outcome we’re modelling. This is especially important when our goal is prediction – it’s not much good if we can only predict well for samples where the outcome is already known, after all!

To get an idea of how well our model generalises, we can split the data into two - a “training” and a “test” set. We use the “training” data to fit the model, and then see its performance on the “test” data.

One thing that often happens in this context is that large coefficient values minimise the training error, but they don’t minimise the test error on unseen data. First, we’ll go through an example of what exactly this means.

To compare the training and test errors for a model of methylation features and age, we’ll split the data into training and test sets, fit a linear model and calculate the errors. First, let’s calculate the training error. Let’s start by splitting the data into training and test sets:

```{r,warning=FALSE, message=FALSE, echo = TRUE}
methylation <- readRDS(here::here("data/methylation.rds"))

library("SummarizedExperiment")
age <- methylation$Age
methyl_mat <- t(assay(methylation))
```

Subset significant CpGs:

```{r,warning=FALSE, message=FALSE, echo = TRUE}
cpg_markers <- c("cg16241714", "cg14424579", "cg22736354", "cg02479575", "cg00864867", 
    "cg25505610", "cg06493994", "cg04528819", "cg26297688", "cg20692569", 
    "cg04084157", "cg22920873", "cg10281002", "cg21378206", "cg26005082", 
    "cg12946225", "cg25771195", "cg26845300", "cg06144905", "cg27377450"
)

horvath_mat <- methyl_mat[, cpg_markers]

## Generate an index to split the data
set.seed(42)
train_ind <- sample(nrow(methyl_mat), 25)

## Split the data 
train_mat <- horvath_mat[train_ind, ]
train_age <- age[train_ind]
test_mat <- horvath_mat[-train_ind, ]
test_age <- age[-train_ind]
```

Now let’s fit a linear model to our training data and calculate the training error. Here we use the mean of the squared difference between our predictions and the observed data, or “mean squared error” (MSE).


```{r,warning=FALSE, message=FALSE, echo = TRUE}
## Fit a linear model
# as.data.frame() converts train_mat into a data.frame
# Using the `.` syntax above together with a `data` argument will lead to
# the same result as using `train_age ~ train_mat`: R will fit a multivariate 
# regression model in which each of the columns in `train_mat` is used as 
# a predictor. We opted to use the `.` syntax because it will help us to 
# obtain model predictions using the `predict()` function. 

fit_horvath <- lm(train_age ~ ., data = as.data.frame(train_mat))

## Function to calculate the (mean squared) error
mse <- function(true, prediction) { 
    mean((true - prediction)^2) 
} 

## Calculate the training error 
err_lm_train <- mse(train_age, fitted(fit_horvath)) 
err_lm_train
```

### Challenge 3

1. For the fitted model above, calculate the mean squared error for the test set.

First, let’s find the predicted values for the ‘unseen’ test data:
```{r,warning=FALSE, message=FALSE, echo = TRUE}
pred_lm <- predict(fit_horvath, newdata = as.data.frame(test_mat)) 

```

The mean squared error for the test set is the mean of the squared error between the predicted values and true test data.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
err_lm <- mse(test_age, pred_lm)
err_lm

```

Unfortunately, the test error is a lot higher than the training error. If we plot true age against predicted age for the samples in the test set, we can gain more insight into the performance of the model on the test set. Ideally, the predicted values should be close to the test data.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
par(mfrow = c(1, 1))
plot(test_age, pred_lm, pch = 19)
abline(coef = 0:1, lty = "dashed")
```

### Ridge regression

Ridge regression adds a penalty to the sum of squared coefficients (L2 norm) to control model complexity. This penalty, scaled by a factor λ, balances reducing large coefficients and fitting the data.

\[
\sum_{i=1}^N (y_i - x'_i \beta)^2 + \lambda \|\beta\|_2^2
\]

- Whenλ is large: Large coefficients are heavily penalized, simplifying the model.
- When λ is small: Coefficients shrink less, resulting in a more complex model.
- When λ = 0: Ridge regression reduces to ordinary least squares.

We'll use the glmnet package to compare regularized and ordinary least squares models using a subset of 20 features (`cpg_markers`) identified earlier.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
library("glmnet")

## glmnet() performs scaling by default, supply un-scaled data:
horvath_mat <- methyl_mat[, cpg_markers] # select the same 20 sites as before
train_mat <- horvath_mat[train_ind, ] # use the same individuals as selected before
test_mat <- horvath_mat[-train_ind, ]

ridge_fit <- glmnet(x = train_mat, y = train_age, alpha = 0)
plot(ridge_fit, xvar = "lambda")
abline(h = 0, lty = "dashed")
```
Since we split the data into test and training data, we can prove that ridge regression predicts the test data better than the model with no regularisation. Let’s generate our predictions under the ridge regression model and calculate the mean squared error in the test set:

```{r,warning=FALSE, message=FALSE, echo = TRUE}

# Obtain a matrix of predictions from the ridge model,
# where each column corresponds to a different lambda value
pred_ridge <- predict(ridge_fit, newx = test_mat)

# Calculate MSE for every column of the prediction matrix against the vector of true ages
err_ridge <- apply(pred_ridge, 2, function(col) mse(test_age, col)) 
min_err_ridge <- min(err_ridge)

# Identify the lambda value that results in the lowest MSE (ie, the "best" lambda value)
which_min_err <- which.min(err_ridge)
pred_min_ridge <- pred_ridge[, which_min_err]

## Return errors
min_err_ridge
```

This is much lower than the test error for the model without regularisation:

```{r,warning=FALSE, message=FALSE, echo = TRUE}
err_lm 
```

We can see where on the continuum of lambdas we’ve picked a model by plotting the coefficient paths again. In this case, we’ve picked a model with fairly modest coefficient shrinking.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
chosen_lambda <- ridge_fit$lambda[which.min(err_ridge)]
plot(ridge_fit, xvar = "lambda")
abline(v = log(chosen_lambda), lty = "dashed")
```

### Challenge 4
1. Which performs better, ridge or OLS?
2. Plot predicted ages for each method against the true ages. How do the predictions look for both methods? Why might ridge be performing better?


1. Ridge regression performs significantly better on unseen data, despite being “worse” on the training data.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
min_err_ridge
err_lm


```

2. The ridge ones are much less spread out with far fewer extreme predictions.


```{r,warning=FALSE, message=FALSE, echo = TRUE}
all <- c(pred_lm, test_age, pred_min_ridge)
lims <- range(all)
par(mfrow = 1:2)
plot(test_age, pred_lm,
    xlim = lims, ylim = lims,
    pch = 19
)
abline(coef = 0:1, lty = "dashed")
plot(test_age, pred_min_ridge,
    xlim = lims, ylim = lims,
    pch = 19
)
abline(coef = 0:1, lty = "dashed")

```

### LASSO Regularization

LASSO regularization uses the **L1 norm** (sum of absolute coefficient values) to shrink coefficients:

\[
\| \beta \|_1 = \sum_{j=1}^p |\beta_j|
\]

This approach encourages **sparse models**, removing unnecessary features by shrinking some coefficients exactly to zero. The sharp boundaries of the restricted region make it more likely that solutions lie at corners, resulting in simpler models.

### Cross-validation to find the best value of  λ

To balance model complexity and information retention, we choose an optimal λ. A common method is cross-validation, which splits the data into K chunks. K−1 chunks are used for training, and the remaining chunk is for testing. This process rotates through all chunks, providing a reliable estimate of how well each λ value generalizes to new data.

We can use this new idea to choose a lambda value by finding the lambda that minimises the error across each of the test and training splits. In R:

```{r,warning=FALSE, message=FALSE, echo = TRUE}
# fit lasso model with cross-validation across a range of lambda values
lasso <- cv.glmnet(methyl_mat, age, alpha = 1)
plot(lasso)

# Extract the coefficients from the model with the lowest mean squared error from cross-validation
coefl <- coef(lasso, lasso$lambda.min)
# select only non-zero coefficients
selection <- which(coefl != 0)
# and convert to a normal matrix
selected_coefs <- as.matrix(coefl)[selection, 1]
selected_coefs
```

We can see that cross-validation has selected a value of λ resulting in 44 features and the intercept.

### Elastic net regression

Elastic net regression combines the properties of ridge (α = 0) and LASSO (α = 1) regression, blending their advantages. It drops uninformative variables like LASSO while maintaining conservative coefficient estimates like ridge, leading to improved predictions.

Elastic net optimizes the following:
\[
\sum_{i=1}^N \left( y_i - x'_i\beta \right)^2 + \lambda \left( \alpha \|\beta\|_1 + (1-\alpha) \|\beta\|_2^2 \right)
\]

- **When α = 1:** Pure LASSO regression (only L1 penalty applies).
- **When α = 0:** Pure ridge regression (only L2 penalty applies).
- **For values between 0 and 1:** A mix of LASSO and ridge properties.

This flexibility allows elastic net to balance variable selection and prediction accuracy. Contour plots help visualize how the penalty changes for different α values.

### Challenge 5
1. Fit an elastic net model (hint: alpha = 0.5) without cross-validation and plot the model object.
2. Fit an elastic net model with cross-validation and plot the error. Compare with LASSO.

1. Fitting an elastic net model is just like fitting a LASSO model. You can see that coefficients tend to go exactly to zero, but the paths are a bit less extreme than with pure LASSO; similar to ridge.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
elastic <- glmnet(methyl_mat[, -1], age, alpha = 0.5)
plot(elastic)
```

The process of model selection is similar for elastic net models as for LASSO models.

```{r,warning=FALSE, message=FALSE, echo = TRUE}
elastic_cv <- cv.glmnet(methyl_mat[, -1], age, alpha = 0.5)
plot(elastic_cv)
```

